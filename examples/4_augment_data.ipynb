{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "academic-salon",
   "metadata": {},
   "source": [
    "# Augment dataset\n",
    "\n",
    "In this notebook we exemplify how to augment a dataset. This is tipically done to increase the number and/or representativity of trainining sets.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Load Dataset](#loadData)\n",
    "    1. [GP Path](#oriGpPath)\n",
    "3. [Augment Dataset](#augData)\n",
    "    1. [Choose the Events to Augment](#chooseEvent)\n",
    "    3. [Choose the Photometric Redshift](#choosePhotoZ)\n",
    "    4. [Run Augmentation](#aug)\n",
    "    5. [See Augmented Dataset Properties](#statsAug)\n",
    "4. [Save Augmented Dataset](#saveAug)\n",
    "5. [Light curve comparison](#comparison)\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snmachine import snaugment, sndata\n",
    "from utils.plasticc_pipeline import get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-colonial",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=1.3, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sheet",
   "metadata": {},
   "source": [
    "## 2. Load Dataset<a name=\"loadData\"></a>\n",
    "\n",
    "First, **write** the path to the folder that contains the dataset we want to augment, `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../snmachine/example_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-estate",
   "metadata": {},
   "source": [
    "Then, **write** in `data_file_name` the name of the file where your dataset is saved.\n",
    "\n",
    "In this notebook we use the dataset saved in [2_preprocess_data]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name = 'example_dataset_gapless50.pckl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-valuation",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-letters",
   "metadata": {},
   "source": [
    "### 2.1. GP Path<a name=\"oriGpPath\"></a>\n",
    "\n",
    "The GP augmentation uses the previously saved GPs, so **write** the path where they were saved. For help in fitting GPs to the dataset, follow [3_model_lightcurves](3_model_lightcurves.ipynb).\n",
    "\n",
    "**<font color=Orange>A)</font>** Obtain GP path from folder structure.\n",
    "\n",
    "If you created a folder structure, you can obtain the path from there. **Write** the name of the folder in `analysis_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = data_file_name[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-citizenship",
   "metadata": {},
   "source": [
    "Obtain the required GP path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = get_directories(folder_path, analysis_name) \n",
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-server",
   "metadata": {},
   "source": [
    "**<font color=Orange>B)</font>** Directly **write** where you saved the GP files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-miami",
   "metadata": {},
   "source": [
    "```python\n",
    "path_saved_gps = os.path.join(folder_path, data_file_name[:-5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-prophet",
   "metadata": {},
   "source": [
    "## 3. Augment Dataset<a name=\"augData\"></a>\n",
    "\n",
    "Here we augment the data and make sure all the properties have the expected values.\n",
    "\n",
    "In the following sections we decide the following augmentation inputs: \n",
    "1. `objs_number_to_aug` : a dictionary specifying which events to augment and by how much.\n",
    "2. `choose_z` : function used to choose the new true redshift of the augmented events.\n",
    "3. `z_table` : dataset containing the spectroscopic and photometric redshift, and photometric redshift error of events; it is used to generate realistic augmented photometric redshifts.\n",
    "4. `max_duration` : maximum duration of the augmented light curves.\n",
    "5. `random_seed` : random seed used; saving this seed allows reproducible results.\n",
    "\n",
    "### 3.1. Choose the Events to Augment<a name=\"chooseEvent\"></a>\n",
    "\n",
    "**Write** in `aug_obj_names` a list containing all the events to augment. Here we will try to augment them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_obj_names = dataset.object_names  # try to augment all events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-raising",
   "metadata": {},
   "source": [
    "**Create** a dictionary that associates to each event, the target number of synthetic events to create from it. Note that some augmentations will fail so this is not the final number of events. Additionally, each class has a different creation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "is_to_aug = np.in1d(dataset.object_names, aug_obj_names)\n",
    "\n",
    "# Choose the target number of events in the augmented dataset. \n",
    "# Usually, only half of this number are accepted in the augmented dataset\n",
    "target_number_aug = np.sum(is_to_aug) * 2\n",
    "\n",
    "number_objs_per_label = collections.Counter(dataset.labels[is_to_aug])\n",
    "number_aug_per_label = target_number_aug//len(number_objs_per_label.keys())\n",
    "objs_number_to_aug = {}\n",
    "for label in number_objs_per_label.keys():\n",
    "    is_label = dataset.labels[is_to_aug] == label\n",
    "    aug_is_label_obj_names = aug_obj_names[is_label]\n",
    "    number_aug_per_obj = number_aug_per_label // np.sum(is_label)\n",
    "    number_extra_aug_per_obj = number_aug_per_label % np.sum(is_label)\n",
    "    extra_obj = np.random.choice(aug_is_label_obj_names, size=number_extra_aug_per_obj, \n",
    "                                 replace=False)\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj for obj in aug_is_label_obj_names})\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj+1 for obj in extra_obj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We aim to create up to {sum(objs_number_to_aug.values())} events.')  # confirm how many events to create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-privilege",
   "metadata": {},
   "source": [
    "### 3.2. Choose the Photometric Redshift<a name=\"choosePhotoZ\"></a>\n",
    "\n",
    "In order to simulate realistic photometric redshifts for the synthetic events, following [Boone (2019)](https://iopscience.iop.org/article/10.3847/1538-3881/ab5182) we chose a random event from the test set events that had a spectroscopic redshift measurement, and calculated the difference between its spectroscopic and photometric redshifts. We then added this difference to the true redshift of the augmented event to generate a photometric redshift. \n",
    "\n",
    "**Add** such a dataset containing spectroscopic and photometric redshift, and photometric redshift error of events as `z_table`. If none is provided, a similar table is generated from the events in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_name = 'example_test_dataset.pckl'\n",
    "test_data_path = os.path.join(folder_path, test_data_file_name)\n",
    "\n",
    "test_data = load_dataset(test_data_path)\n",
    "test_metadata = test_data.metadata\n",
    "\n",
    "# Discard the events without spectroscopic redshift; \n",
    "# these are encoded with `hostgal_specz` equal to -9\n",
    "z_table = test_metadata[test_metadata.hostgal_specz > -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-meditation",
   "metadata": {},
   "source": [
    "### 3.3. Run Augmentation<a name=\"aug\"></a>\n",
    "\n",
    "We also need to choose which survey to emulate in the augmentation. At the moment `snmachine` contains the Wide-Fast-Deep (WFD) and the Deep Drilling Field (DDF) survey of the Rubin Observatory Legacy Survey of Space and Time. Use `snaugment.PlasticcWFDAugment` for the former survey and `snaugment.PlasticcDDFAugment` for the latter.\n",
    "You can also implement your own augmentation using those classes as an example.\n",
    "\n",
    "In addition to the above inputs, we **chose** the random seed (`random_seed`) used to allow reproducible results and the maximum duration of the augmented light curves (`max_duration`).\n",
    "\n",
    "The value of `max_duration` must be higher than the maximum duration of any light curve in `dataset`. If none is provided, `max_duration` is set to the length of the longest event in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest event in `dataset` has {dataset.get_max_length():.2f} days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 \n",
    "max_duration = 277  # this is the length of the longest event in the PLAsTiCC SNe dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-resolution",
   "metadata": {},
   "source": [
    "Here we augmented following the WFD survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = snaugment.PlasticcWFDAugment(dataset=dataset, path_saved_gps=path_saved_gps, \n",
    "                                   objs_number_to_aug=objs_number_to_aug,\n",
    "                                   random_seed=random_seed, max_duration=max_duration, \n",
    "                                   z_table=z_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug.augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-crowd",
   "metadata": {},
   "source": [
    "Go to:\n",
    "* [Index](#index)\n",
    "* [Save Augmented Dataset](#saveAug)\n",
    "  \n",
    "### 3.4. See Augmented Dataset Properties<a name=\"statsAug\"></a>\n",
    "\n",
    "Here we see some properties of the augmented dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # a test set was provided\n",
    "    test_metadata['target'] = test_metadata['true_target']\n",
    "    datasets_label = ['Original', 'Only Aug.', 'Test data']\n",
    "    datasets_metadata = [aug.dataset.metadata, aug.only_new_dataset.metadata, test_metadata]\n",
    "except NameError:   # no test set was provided\n",
    "    datasets_label = ['Original', 'Only Aug.']\n",
    "    datasets_metadata = [aug.dataset.metadata, aug.only_new_dataset.metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest event in the augmented dataset (`aug.only_new_dataset`)'\n",
    "      f' has {aug.only_new_dataset.get_max_length():.2f} days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'In total we generated {len(aug.only_new_dataset.object_names)} events.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-grade",
   "metadata": {},
   "source": [
    "Note that we generated less events than our target number of augmented events. As mentioned in Section [Choose the Events to Augment](#chooseEvent), some of the augmentations fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:^12}  {:^12}  {:^12}  {:^12}'.format('Dataset', 'total # objs', '# DDF objs', '% DDF objs'))\n",
    "print('-'*(12*4 + 3*2))\n",
    "for i in np.arange(len(datasets_label)):\n",
    "    is_ddf = datasets_metadata[i]['ddf'] == 1\n",
    "    number_total_objs = len(is_ddf)\n",
    "    number_ddf_objs = np.sum(is_ddf)\n",
    "    print('{:^12} {:^12} {:^12} {:^12.2f}'.format(\n",
    "        datasets_label[i], number_total_objs, number_ddf_objs, \n",
    "        number_ddf_objs/number_total_objs * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-northeast",
   "metadata": {},
   "source": [
    "We now see the distribution of the photometric redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-ferry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting choices\n",
    "diverg_color = sns.color_palette(\"Set2\", 3, desat=1)\n",
    "sn_type_color = {42: diverg_color[1], 62: diverg_color[0], 90: diverg_color[2]}\n",
    "sn_type_name = {42: 'SN II', 62: 'SN Ibc', 90: 'SN Ia'}\n",
    "unique_types = [90, 42, 62]\n",
    "datasets_ls = ['-', '-', '--']\n",
    "datasets_linewidth = [1, 3, 3]\n",
    "datasets_bw_adjust = [.3, .8, .8]\n",
    "\n",
    "# Plot the redshift distribution\n",
    "for sn_type in unique_types: # sns scale 2\n",
    "    plt.figure()\n",
    "    for i, metadata in enumerate(datasets_metadata):\n",
    "        label = datasets_label[i]\n",
    "        ls = datasets_ls[i]\n",
    "        linewidth = datasets_linewidth[i]\n",
    "        bw_adjust= datasets_bw_adjust[i]\n",
    "        is_sn_type = (metadata['target'] == sn_type)\n",
    "        sn_type_metadata = metadata[is_sn_type]\n",
    "        try:\n",
    "            sns.kdeplot(data=sn_type_metadata['hostgal_photoz'],\n",
    "                        label=label, color=sn_type_color[sn_type],\n",
    "                        linestyle=ls, linewidth=linewidth, \n",
    "                        bw=bw_adjust)\n",
    "        except TypeError:  # outdated version of matplotlib\n",
    "            sns.distplot(a=sn_type_metadata['hostgal_photoz'], \n",
    "                         label=label, color=sn_type_color[sn_type],\n",
    "                         kde_kws={'linestyle': ls, \n",
    "                                  'linewidth': linewidth, \n",
    "                                  'bw': bw_adjust})\n",
    "    sn_name = sn_type_name[sn_type]\n",
    "    plt.title(sn_name)\n",
    "    plt.xlim(-.1, 1.5)\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlabel('Photometric redshift')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(handletextpad=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-satellite",
   "metadata": {},
   "source": [
    "## 4. Save Augmented Dataset<a name=\"saveAug\"></a>\n",
    "\n",
    "Now, we save the `PlasticcData` instance containing only the augmented events. **Chose** a path to save (`folder_path_to_save`) and the name of the file (`file_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_to_save = folder_path\n",
    "file_name = 'example_dataset_aug.pckl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-cherry",
   "metadata": {},
   "source": [
    "At this point we could also choose to save only part of the augmented dataset. Here we save all the augmented events.\n",
    "\n",
    "**Add** an extra step to select your chosen subset. See the notebook [1_load_data](1_load_data.ipynb) for a tutorial on how to select a subset from a `PlasticcData` instance. This can be used, for example, to create an augmented training set with the same number of events in each class.\n",
    "\n",
    "Finally, save the `PlasticcData` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_aug_dataset = aug.only_new_dataset\n",
    "\n",
    "path_to_save = os.path.join(folder_path_to_save, file_name)\n",
    "with open(path_to_save, 'wb') as f:\n",
    "    pickle.dump(only_aug_dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-elite",
   "metadata": {},
   "source": [
    "## 5. Light curve visualization<a name=\"see\"></a>\n",
    "\n",
    "Here we show the light curve of an event along with one of the synthetic events generated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_show = '47712228'\n",
    "sndata.PlasticcData.plot_obj_and_model(dataset.data[obj_show])\n",
    "plt.title(f'Event {obj_show}; z = {dataset.data[obj_show].meta[\"z\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_aug_show = obj_show + '_aug1'\n",
    "sndata.PlasticcData.plot_obj_and_model(only_aug_dataset.data[obj_aug_show])\n",
    "plt.title(f'Event {obj_aug_show}; z = {only_aug_dataset.data[obj_aug_show].meta[\"z\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-concentrate",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "*Previous notebook:* [3_model_lightcurves](3_model_lightcurves.ipynb)\n",
    "\n",
    "**Next notebook:** [5_feature_extraction](5_feature_extraction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
