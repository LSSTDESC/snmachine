{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "great-athletics",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "In this notebook we follow the wavelet decomposition approach of [Lochner et al. (2016)](https://iopscience.iop.org/article/10.3847/0067-0049/225/2/31) to extract features. We also include the photometric redshift and its uncertainty as classification features.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Load Dataset](#loadData)\n",
    "3. [Extract Features](#features)\n",
    "    1. [Fit Gaussian Processes](#gps)\n",
    "    2. [Wavelet Decomposition](#waveletDecomp)\n",
    "    3. [Include Redshift Information](#addZ)\n",
    "    4. [Save the Features](#saveFeatures)\n",
    "    5. [Load Features](#load)\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "duplicate-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "global-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'pymultinest'\n",
      "\n",
      "                PyMultinest not found. If you would like to use, please install\n",
      "                Mulitnest with 'sh install/multinest_install.sh; source\n",
      "                install/setup.sh'\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "from snmachine import gps, snfeatures\n",
    "from utils.plasticc_pipeline import create_folder_structure, get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graduate-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-fiber",
   "metadata": {},
   "source": [
    "## 2. Load Dataset<a name=\"loadData\"></a>\n",
    "\n",
    "First, **write** the path to the folder that contains the dataset we want to augment, `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appreciated-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../snmachine/example_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-lecture",
   "metadata": {},
   "source": [
    "Then, **write** in `data_file_name` the name of the file where your dataset is saved.\n",
    "\n",
    "In this notebook we use the dataset saved in [4_augment_data](4_augment_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "satellite-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name = 'example_dataset_aug.pckl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-attention",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "colonial-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening from binary pickle\n",
      "Dataset loaded from pickle file as: <snmachine.sndata.PlasticcData object at 0x7faa88591b10>\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-struggle",
   "metadata": {},
   "source": [
    "## 3. Extract Features<a name=\"features\"></a>\n",
    "\n",
    "### 3.1. Fit Gaussian Processes<a name=\"gps\"></a>\n",
    "\n",
    "To obtain the wavelet decomposition, we first used the GPs to interpolate all light curves onto the same time grid; we chose approximately one grid point per day and used a two-level wavelet decomposition, following [Lochner et al. (2016)](https://iopscience.iop.org/article/10.3847/0067-0049/225/2/31).\n",
    "\n",
    "If you have not fitted the GPs previously, **run** **<font color=Orange>A2)</font>**; it follows the GP modeling of light curves described in [3_model_lightcurves]().\n",
    "Otherwise, follow **<font color=Orange>B2)</font>** to **read in** the previously saved GPs. \n",
    "\n",
    "First **write** the path to the folder where the GP files will be/were saved (`path_saved_gps`). Similarly to previous notebooks, you can opt:\n",
    "\n",
    "**<font color=Orange>A1)</font>** Obtain GP path from folder structure.\n",
    "\n",
    "If you created a folder structure, you can obtain the path from there. **Write** the name of the folder in `analysis_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rubber-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = data_file_name[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-corporation",
   "metadata": {},
   "source": [
    "Create the folder structure, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "early-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Folders already exist with this analysis name.\n",
      "\n",
      "                Are you sure you would like to proceed, this will overwrite the\n",
      "                example_dataset_aug folder [Y/n]\n",
      "                \n",
      "\n",
      "Please respond with 'yes' or 'no'"
     ]
    }
   ],
   "source": [
    "create_folder_structure(folder_path, analysis_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-village",
   "metadata": {},
   "source": [
    "Obtain the required GP path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "homeless-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = get_directories(folder_path, analysis_name) \n",
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-ireland",
   "metadata": {},
   "source": [
    "**<font color=Orange>A2)</font>** Directly **write** where you saved the GP files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-clinton",
   "metadata": {},
   "source": [
    "```python\n",
    "path_saved_gps = os.path.join(folder_path, data_file_name[:-5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-uruguay",
   "metadata": {},
   "source": [
    "**<font color=Orange>B1)</font>** **Choose**:\n",
    "- `t_min`: minimim time to evaluate the Gaussian Process Regression at.\n",
    "- `t_max`: maximum time to evaluate the Gaussian Process Regression at.\n",
    "- `gp_dim`: dimension of the Gaussian Process Regression. If  `gp_dim` is 1, the filters are fitted independently. If `gp_dim` is 2, the Matern kernel is used to fit light curves both in time and wavelength.\n",
    "- `number_gp`: number of points to evaluate the Gaussian Process Regression at.\n",
    "- `number_processes`: number of processors to use for parallelisation (**<font color=green>optional</font>**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifteen-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = 0\n",
    "t_max = 278\n",
    "\n",
    "gp_dim = 2\n",
    "number_gp = 276\n",
    "number_processes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "existing-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Gaussian process regression.\n",
      "Models fitted with the Gaussian Processes values.\n",
      "Time taken for Gaussian process regression: 5.58s.\n"
     ]
    }
   ],
   "source": [
    "gps.compute_gps(dataset, number_gp=number_gp, t_min=t_min, t_max=t_max, \n",
    "                gp_dim=gp_dim, output_root=path_saved_gps, \n",
    "                number_processes=number_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-adams",
   "metadata": {},
   "source": [
    "**<font color=Orange>B2)</font>** Read in the previously saved GPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-portsmouth",
   "metadata": {},
   "source": [
    "```python\n",
    "gps.read_gp_files_into_models(dataset, saved_gps_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-recovery",
   "metadata": {},
   "source": [
    "### 3.2. Wavelet Decomposition<a name=\"waveletDecomp\"></a>\n",
    "\n",
    "Now, we do a wavelet decomposition of the events. **Write** in `path_saved_wavelets` the path to the folder where to save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acquired-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_wavelets = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-lotus",
   "metadata": {},
   "source": [
    "Following [Lochner et al. (2016)](https://iopscience.iop.org/article/10.3847/0067-0049/225/2/31), we then reduced the dimensionality of this wavelet space using Principal Component Analysis (PCA). Therefore, **choose** the number of PCA components to keep (`number_comps`) and **write** the path to the folder where to save the reduced wavelets (`path_saved_reduced_wavelets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "union-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_comps = 40\n",
    "path_saved_reduced_wavelets = directories['features_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-mortality",
   "metadata": {},
   "source": [
    "**<font color=Orange>A)</font>** Perform the wavelet decomposition and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cubic-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wavelet used is sym2.\n",
      "Each passband is decomposed in 2 levels.\n",
      "Performing wavelet decomposition.\n",
      "Time taken for wavelet decomposition: 0.92s.\n",
      "Performing eigendecomposition.\n",
      "Time taken for eigendecomposition: 0.19s.\n",
      "Dimensionality reduced feature space with 40 components.\n"
     ]
    }
   ],
   "source": [
    "wf = snfeatures.WaveletFeatures(output_root=path_saved_wavelets)\n",
    "\n",
    "reduced_wavelet_features = wf.compute_reduced_features(\n",
    "    dataset, number_comps=number_comps, \n",
    "    **{'wavelet_name': 'sym2', 'number_decomp_levels': 2,\n",
    "       'path_save_eigendecomp': path_saved_reduced_wavelets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latin-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../snmachine/example_data/example_dataset_aug/wavelet_features'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_saved_reduced_wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-monaco",
   "metadata": {},
   "source": [
    "If you previously calculated the wavelet decomposition of the events, and are only looking to project them into a lower dimensional space saved in `path_saved_reduced_wavelets`, run **<font color=Orange>B)</font>**.\n",
    "\n",
    "**<font color=Orange>B)</font>** Project previously calculated wavelet features onto a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-romantic",
   "metadata": {},
   "source": [
    "```python\n",
    "wf = snfeatures.WaveletFeatures(output_root=saved_wavelets_path)\n",
    "feature_space = wf.load_feature_space(dataset)\n",
    "\n",
    "reduced_wavelet_features = wf.project_to_space(\n",
    "    feature_space, path_saved_eigendecomp=saved_reduced_wavelets_path,\n",
    "    number_comps=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-honor",
   "metadata": {},
   "source": [
    "Save the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southwest-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.save_reduced_features(reduced_wavelet_features, path_saved_reduced_wavelets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-camera",
   "metadata": {},
   "source": [
    "### 3.3. Include Redshift Information<a name=\"addZ\"></a>\n",
    "\n",
    "In [paper]() we found that photometric redshift and its uncertainty are crucial for classification. Therefore, in the cell bellow, we include these properties as features. **Modify** it to include other properties as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opponent-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduced_wavelet_features.copy()  # only the wavelet features\n",
    "\n",
    "metadata = dataset.metadata\n",
    "features['hostgal_photoz'] = metadata.hostgal_photoz.values.astype(float)\n",
    "features['hostgal_photoz_err'] = metadata.hostgal_photoz_err.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-doctor",
   "metadata": {},
   "source": [
    "### 3.4. Save the Features<a name=\"saveFeatures\"></a>\n",
    "\n",
    "**Write** in `saved_features_path` the path to the folder where to save the final set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pending-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_features = directories['features_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-vietnamese",
   "metadata": {},
   "source": [
    "Save the features and the class of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flexible-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_pickle(os.path.join(path_saved_features, 'features.pckl'))\n",
    "\n",
    "data_labels = dataset.labels.astype(int)  # class label of each event\n",
    "data_labels.to_pickle(os.path.join(path_saved_features, 'data_labels.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-testing",
   "metadata": {},
   "source": [
    "### 3.5. Load Features<a name=\"load\"></a> <font color=salmon>(Optional)</font>\n",
    "\n",
    "We can load the saved files to verify weather they were correctly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "inner-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_features = pd.read_pickle(os.path.join(path_saved_features, 'features.pckl')) \n",
    "saved_data_labels = pd.read_pickle(os.path.join(path_saved_features, 'data_labels.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-finder",
   "metadata": {},
   "source": [
    "As we can see, the quantities are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "engaging-tanzania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(saved_features, features))\n",
    "print(np.allclose(saved_data_labels, data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-fifty",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "*Previous notebook:* [4_augment_data](4_augment_data.ipynb)\n",
    "\n",
    "**Next notebook:** [6_train_classifier](6_train_classifier.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
