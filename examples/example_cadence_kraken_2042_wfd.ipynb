{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running the snmachine pipeline on PLAsTiCC simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of the `snmachine` supernova classification package by classifying a subset simulated data from the photometric light-curve astronomical time-series classification challenge (PLAsTiCC). \n",
    "\n",
    "See Lochner et al. (2016) http://arxiv.org/abs/1603.00882 for the original SPCC-challenge test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the how the pipeline works. As the user, you can choose what feature extraction method you want to use. Here we have three (four, technically, since there are two parametric models) but it's straighforward to write a new feature extraction method. Once features have been extracted, they can be run through one of several machine learning algorithms and again, it's easy to write your own algorithm into the pipeline. There's a convenience function in `snclassifier` to run a feature set through multiple algorithms and plot the result. The rest of this notebook goes through applying each of the feature extraction methods to a set of simulations and running all feature sets through different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout \n",
    "#I use this to supress unnecessary warnings for clarity\n",
    "%load_ext autoreload\n",
    "%autoreload #Use this to reload modules if they are changed on disk while the notebook is running\n",
    "from snmachine import sndata, snfeatures, snclassifier, tsne_plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, pywt,subprocess\n",
    "from sklearn.decomposition import PCA\n",
    "from astropy.table import Table,join,vstack,unique\n",
    "from astropy.io import fits\n",
    "import sklearn.metrics \n",
    "import sncosmo\n",
    "import pickle\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload #Use this to reload modules if they are changed on disk while the notebook is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of processes you want to use throughout the notebook\n",
    "import multiprocessing\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "nproc=num_cpu\n",
    "print(\"Running with {} cores\".format(num_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up output structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make lots of output files so it makes sense to put them in one place. This is the recommended output file structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a subset of the PLAsTiCC simulated data (https://arxiv.org/abs/1810.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please specify Data root, \n",
    "# the path to where you have pulled all the data from\n",
    "rt='/share/hypatia/snmachine_resources/data/cwp/WFDY10/RH_kraken_2042_wfd_WFD_1aONLY_Y10_G10/'\n",
    "prefixIa='RH_WFD_1aONLY_Y10_G10_Ia-'\n",
    "prefixNONIa='RH_WFD_1aONLY_Y10_G10_NONIa-'\n",
    "# Name for the dataset\n",
    "dataset='kraken_2042_wfd_Y10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING...\n",
    "#Multinest uses a hardcoded character limit for the output file names. I believe it's a limit of 100 characters\n",
    "#so avoid making this file path to lengthy if using nested sampling or multinest output file names will be truncated\n",
    "\n",
    "#Change outdir to somewhere on your computer if you like\n",
    "# outdir=os.path.join('output_{}_no_z'.format(dataset),'')\n",
    "outdir=\"/share/hypatia/snmachine_resources/data/LSST_Cadence_WhitePaperClassResults/output_data/revision/output_{}_no_z/\".format(dataset)\n",
    "\n",
    "out_features=os.path.join(outdir,'features') #Where we save the extracted features to\n",
    "out_class=os.path.join(outdir,'classifications') #Where we save the classification probabilities and ROC curves\n",
    "out_int=os.path.join(outdir,'int') #Any intermediate files (such as multinest chains or GP fits)\n",
    "\n",
    "subprocess.call(['mkdir',outdir])\n",
    "subprocess.call(['mkdir',out_features])\n",
    "subprocess.call(['mkdir',out_class])\n",
    "subprocess.call(['mkdir',out_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "chunks = random.sample(range(1, 21), 1)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat=sndata.LSSTCadenceSimulations(folder=rt,prefix_Ia=prefixIa, prefix_NONIa=prefixNONIa, indices=chunks)\n",
    "#dat=sndata.plasticc_data(folder=rt,pickle_file='dataset_full.pickle',from_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can plot all the data and cycle through it (left and right arrows on your keyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.plot_all(mix=True, sep_detect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types, note these are internal snmachine datatypes\n",
    "types=dat.get_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each light curve is represented in the Dataset object as an astropy table, compatible with `sncosmo`:\n",
    "\n",
    "Note: The types listed here in the table the internal types to snmachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, reds=dat.get_redshift()\n",
    "reds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object names from data\n",
    "# total_objects = dat.object_names[:]\n",
    "# print(total_objects)\n",
    "# np.random.seed(SEED)\n",
    "# np.random.shuffle(total_objects)\n",
    "# print(total_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this into a Python list\n",
    "# total_set = list(total_objects)\n",
    "# print(type(total_set)) # Should be a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 22000 objects from this Python list of objects, OR chose a\n",
    "# float between 0 and 1 of the percentage of the training set to use.\n",
    "# reduced_set = total_objects[:22000]\n",
    "# with open(outdir+'{}_reduced_set.txt'.format(dataset), 'w') as f:\n",
    "#     for item in reduced_set:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "        \n",
    "subset_file = outdir+'{}_reduced_set.txt'.format(dataset)\n",
    "if os.path.exists(subset_file):\n",
    "    rand_objs = np.genfromtxt(subset_file, dtype='U')\n",
    "else:\n",
    "    np.random.seed(SEED)\n",
    "    rand_objs = np.random.choice(dat.object_names, replace=False, size=22000)\n",
    "    np.savetxt(subset_file, rand_objs, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(reduced_set)\n",
    "# subset = list(map(int, reduced_set))\n",
    "len(rand_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.object_names = rand_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.object_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object names from data\n",
    "new_total_objects = dat.object_names[:]\n",
    "print(len(new_total_objects))\n",
    "np.random.shuffle(new_total_objects)\n",
    "print(new_total_objects)\n",
    "\n",
    "# Turn this into a Python list\n",
    "new_total_set = list(new_total_objects)\n",
    "print(type(new_total_set)) # Should be a list\n",
    "\n",
    "# Now choose a random selection for the training\n",
    "training_set = new_total_set[:2000]\n",
    "len(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect GP fitting capability for individual objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_obj = '3211874' # a nice Ia\n",
    "test_obj = dat.object_names[23]\n",
    "test_obj\n",
    "plt.figure()\n",
    "dat.plot_lc(test_obj, plot_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = dat.data[test_obj]\n",
    "g=snfeatures._GP(test_obj, dat,ngp=100,xmin=0,xmax=dat.get_max_length(),initheta=[500,20], save_output=True, output_root=os.path.join(outdir, 'int', ''))\n",
    "dat.models[test_obj] = g\n",
    "type(dat)\n",
    "#dat.plot_lc(test_obj, plot_model=True)\n",
    "plt.figure()\n",
    "dat.plot_lc(test_obj, plot_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_dict = dat.reduced_chi_squared([test_obj])\n",
    "chi_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract useful features from the data. This can often take a long time, depending on the feature extraction method, so it's a good idea to save these to file (`snmachine` by default saves to astropy tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_file=False #We can use this flag to quickly rerun from saved features\n",
    "run_name=os.path.join(out_features,'{}_all'.format(dataset))\n",
    "read_from_pickle=False\n",
    "pickle_location = rt\n",
    "restart_from_GP = False\n",
    "restart_from_wavefeats=False\n",
    "restart_from_wavelets=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wavelet feature extraction process is quite complicated, although it is fairly fast. Remember to save the PCA eigenvalues, vectors and mean for later reconstruction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveFeats=snfeatures.WaveletFeatures()\n",
    "wavelet_feats=snfeatures.WaveletFeatures(wavelet='sym2', ngp=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stdout\n",
    "if read_from_file:\n",
    "    wave_features=Table.read('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    #Crucial for this format of id's\n",
    "    blah=wave_features['Object'].astype(str)\n",
    "    wave_features.replace_column('Object', blah)\n",
    "    PCA_vals=np.loadtxt('%s_wavelets_PCA_vals.dat' %run_name)\n",
    "    PCA_vec=np.loadtxt('%s_wavelets_PCA_vec.dat' %run_name)\n",
    "    PCA_mean=np.loadtxt('%s_wavelets_PCA_mean.dat' %run_name)\n",
    "elif read_from_pickle:\n",
    "    print('THIS IS NOT CURRENTLY IMPLEMENTED')\n",
    "    f = open(rt)\n",
    "    wave_features=Table.read('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    #Crucial for this format of id's\n",
    "    blah=wave_features['Object'].astype(str)\n",
    "    wave_features.replace_column('Object', blah)\n",
    "    PCA_vals=np.loadtxt('%s_wavelets_PCA_vals.dat' %run_name)\n",
    "    PCA_vec=np.loadtxt('%s_wavelets_PCA_vec.dat' %run_name)\n",
    "    PCA_mean=np.loadtxt('%s_wavelets_PCA_mean.dat' %run_name)\n",
    "\n",
    "elif restart_from_GP:\n",
    "    wave_features=waveFeats.extract_features(dat,nprocesses=nproc,output_root=rt,save_output='all',restart='gp')\n",
    "    wave_features.write('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    np.savetxt('%s_wavelets_PCA_vals.dat' %run_name,waveFeats.PCA_eigenvals)\n",
    "    np.savetxt('%s_wavelets_PCA_vec.dat' %run_name,waveFeats.PCA_eigenvectors)\n",
    "    np.savetxt('%s_wavelets_PCA_mean.dat' %run_name,waveFeats.PCA_mean)\n",
    "    \n",
    "    PCA_vals=waveFeats.PCA_eigenvals\n",
    "    PCA_vec=waveFeats.PCA_eigenvectors\n",
    "    PCA_mean=waveFeats.PCA_mean\n",
    "    \n",
    "elif restart_from_wavefeats:\n",
    "    wave_features=Table.read(rt  + 'wavelet_features.fits',format='fits')\n",
    "    wave_features.write('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    f = open(rt+'PCA_eigenvals.pickle','rb')\n",
    "    PCA_vals=pickle.load(f)\n",
    "    f.close()\n",
    "    f = open(rt+'PCA_eigenvectors.pickle','rb')\n",
    "    PCA_vec=pickle.load(f)\n",
    "    f.close()\n",
    "    f = open(rt+'PCA_mean.pickle','rb')\n",
    "    PCA_mean=pickle.load(f)\n",
    "    f.close()\n",
    "    np.savetxt('%s_wavelets_PCA_vals.dat' %run_name,PCA_vals)\n",
    "    np.savetxt('%s_wavelets_PCA_vec.dat' %run_name,PCA_vec)\n",
    "    np.savetxt('%s_wavelets_PCA_mean.dat' %run_name,PCA_mean)\n",
    "\n",
    "elif restart_from_wavelets:\n",
    "    # RESTART FROM WAVELETS\n",
    "    # Copy int to finaldir and read in raw wavelets\n",
    "    wavelet_feats=snfeatures.WaveletFeatures(wavelet='sym2', ngp=100)\n",
    "    wave_raw, wave_err=wavelet_feats.restart_from_wavelets(dat, os.path.join(outdir, 'int', ''))\n",
    "    wavelet_features,vals,vec,means=wavelet_feats.extract_pca(dat.object_names.copy(), wave_raw)\n",
    "\n",
    "else:\n",
    "    wavelet_features=wavelet_feats.extract_features(dat,nprocesses=nproc,output_root=out_int,save_output='all')\n",
    "    wavelet_features.write('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    np.savetxt('%s_wavelets_PCA_vals.dat' %run_name,wavelet_feats.PCA_eigenvals)\n",
    "    np.savetxt('%s_wavelets_PCA_vec.dat' %run_name,wavelet_feats.PCA_eigenvectors)\n",
    "    np.savetxt('%s_wavelets_PCA_mean.dat' %run_name,wavelet_feats.PCA_mean)\n",
    "    \n",
    "    vals=wavelet_feats.PCA_eigenvals\n",
    "    vec=wavelet_feats.PCA_eigenvectors\n",
    "    means=wavelet_feats.PCA_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat.set_model(waveFeats.fit_sn,wave_features,PCA_vec,PCA_mean,0,dat.get_max_length(),dat.filter_set)\n",
    "dat.set_model(wavelet_feats.fit_sn,wavelet_features,vec,means,0,dat.get_max_length(),dat.filter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.plot_all(mix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to double check the GP's have been fit well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_dict = dat.reduced_chi_squared(training_set)\n",
    "chi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(99)\n",
    "plt.hist(chi_dict.values(), bins=1000, range=(0.0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the machine learning algorithm. There's a utility function in the `snclassifier` library to make it easy to run all the algorithms available, including converting features to `numpy` arrays and rescaling them and automatically generating ROC curves and metrics. Hyperparameters are automatically selected using a grid search combined with cross-validation. All functionality can also be individually run from `snclassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers can be run in parallel, change this parameter to the number of processors on your machine (we're only running 4 algorithms so it won't help to set this any higher than 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available classifiers \n",
    "print(snclassifier.choice_of_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPCC-like pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like for SPCC example notebook where we restrict ourselves to three supernova types:\n",
    "# Ia (1), II (2) and Ibc (3) by carrying out the following pre-proccessing steps\n",
    "types['Type'] = types['Type']-100\n",
    "\n",
    "types['Type'][np.floor(types['Type']/10)==2]=2\n",
    "types['Type'][np.floor(types['Type']/10)==3]=3\n",
    "types['Type'][np.floor(types['Type']/10)==4]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "clss, cms=snclassifier.run_pipeline(wavelet_features,types,output_name=os.path.join(out_class,'wavelets'),\n",
    "                          training_set=training_set, classifiers=['random_forest'], nprocesses=nproc, \n",
    "                            return_classifier=True, classifiers_for_cm_plots='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from astropy.table import Table,join,unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cms[0]\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "annot = np.around(cm, 2)\n",
    "\n",
    "labels=[]\n",
    "for tp_row in unique(types, keys='Type'):\n",
    "    labels.append(tp_row['Type'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "sns.heatmap(cm, xticklabels=labels, yticklabels=labels, cmap='Blues', annot=annot, lw=0.5)\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
