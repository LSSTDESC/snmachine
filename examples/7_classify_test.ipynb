{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indoor-section",
   "metadata": {},
   "source": [
    "# Classify the Test Set\n",
    "\n",
    "In this notebook we used the previously trained Gradient Boosting Decision Tree (see notebook [6_train_classifier](6_train_classifier.ipynb) for how to train it) to classify the test set. Additionally, we show some performance metrics.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Test Set Features](#testFeatures)\n",
    "    1. [Preprocess Dataset](#preprocess)\n",
    "        1. [Load Original Dataset](#loadData)\n",
    "        2. [Preprocess Light Curves](#preprocess)\n",
    "        3. [Save Processed PlasticcData](#saveData)\n",
    "    2. [Extract Features](#features)\n",
    "        1. [Fit Gaussian Processes](#gps)\n",
    "        2. [Wavelet Decomposition](#waveletDecomp)\n",
    "        3. [Include Redshift Information](#addZ)\n",
    "        4. [Save the Features](#saveFeatures)\n",
    "3. [Load Classifier](#loadClassifier)\n",
    "4. [Classify Test Set](#classify)\n",
    "5. [Performance](#performance) <font color=salmon>(Optional)</font>\n",
    "    1. [Metrics](#metrics)\n",
    "    2. [Confusion Matrix](#cm)\n",
    "    3. [ROC Curves](#roc)\n",
    "    4. [Recall and Precision](#recallPrecision)\n",
    "        1. [Recall plot](#recall)\n",
    "        2. [Precision plot](#precision)\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snmachine import snclassifier, gps, snfeatures, analysis\n",
    "from utils.plasticc_pipeline import create_folder_structure, get_directories, load_dataset\n",
    "from utils.plasticc_utils import plot_confusion_matrix, plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-america",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-province",
   "metadata": {},
   "source": [
    "## 2. Test Set Features<a name=\"testFeatures\"></a>\n",
    "\n",
    "Before classifying the test set events, we need to obtain their features. \n",
    "\n",
    "### 2.1. Preprocess Dataset<a name=\"preprocess\"></a>\n",
    "\n",
    "We start by preprocessing the test set similarly to what we did to the training set in [2_preprocess_data](2_preprocess_data.ipynb).\n",
    "\n",
    "#### 2.1.1. Load Original Dataset<a name=\"loadData\"></a>\n",
    "\n",
    "First, **write** the path to the dataset folder `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../snmachine/example_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-noise",
   "metadata": {},
   "source": [
    "Then, **write** in `data_file_name` the name of the file where your dataset is saved.\n",
    "\n",
    "In this notebook we use the test set created in [1_load_data](1_load_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name = 'example_test_dataset.pckl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-horror",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-beijing",
   "metadata": {},
   "source": [
    "If the features were already calculated, jump to [2.2.5. Load the Features](#loadFeatures)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-monitoring",
   "metadata": {},
   "source": [
    "#### 2.1.2. Preprocess light curves<a name=\"preprocess\"></a>\n",
    "\n",
    "**Write** the maximum duration of the gap to allowed in the light curves, `max_gap_length`. Note that this value *must* be the same as the one used for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gap_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_gaps(max_gap_length*2, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length*2, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest processed light curve has {dataset.get_max_length():.2f} days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-charger",
   "metadata": {},
   "source": [
    "#### 2.1.3. Save Processed PlasticcData<a name=\"saveData\"></a>\n",
    "\n",
    "Now, **chose** a path to save the PlasticcData instance created (`folder_path_to_save`) and the name of the file (`file_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_to_save = folder_path\n",
    "file_name = data_file_name[:-5]+'_gapless50.pckl'\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder_path_to_save, file_name), 'wb') as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-myrtle",
   "metadata": {},
   "source": [
    "### 2.2. Extract Features<a name=\"features\"></a>\n",
    "\n",
    "Here we mostly repeat the steps shown in [5_feature_extraction](5_feature_extraction.ipynb).\n",
    "\n",
    "#### 2.2.1. Fit Gaussian Processes<a name=\"gps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-billion",
   "metadata": {},
   "source": [
    "First **write** the path to the folder where the GP files will be/were saved (`path_saved_gps`). Similarly to previous notebooks, you can opt:\n",
    "\n",
    "**<font color=Orange>A1)</font>** Obtain GP path from folder structure.\n",
    "\n",
    "If you created a folder structure, you can obtain the path from there. **Write** the name of the folder in `analysis_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = file_name[:-5]\n",
    "analysis_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-riverside",
   "metadata": {},
   "source": [
    "Create the folder structure, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_structure(folder_path, analysis_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-questionnaire",
   "metadata": {},
   "source": [
    "Obtain the required GP path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = get_directories(folder_path, analysis_name) \n",
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-oriental",
   "metadata": {},
   "source": [
    "**<font color=Orange>A2)</font>** Directly **write** where you saved the GP files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-motivation",
   "metadata": {},
   "source": [
    "```python\n",
    "path_saved_gps = os.path.join(folder_path, data_file_name[:-5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-improvement",
   "metadata": {},
   "source": [
    "**<font color=Orange>B)</font>** **Choose**:\n",
    "- `t_min`: minimim time to evaluate the Gaussian Process Regression at.\n",
    "- `t_max`: maximum time to evaluate the Gaussian Process Regression at.\n",
    "- `gp_dim`: dimension of the Gaussian Process Regression. If  `gp_dim` is 1, the filters are fitted independently. If `gp_dim` is 2, the Matern kernel is used to fit light curves both in time and wavelength.\n",
    "- `number_gp`: number of points to evaluate the Gaussian Process Regression at.\n",
    "- `number_processes`: number of processors to use for parallelisation (**<font color=green>optional</font>**).\n",
    "\n",
    "*Note* that all the above parameters with the exception of `number_processes` *must* be the same as the ones used for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = 0\n",
    "t_max = 277\n",
    "\n",
    "gp_dim = 2\n",
    "number_gp = 276\n",
    "number_processes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps.compute_gps(dataset, number_gp=number_gp, t_min=t_min, t_max=t_max, \n",
    "                gp_dim=gp_dim, output_root=path_saved_gps, \n",
    "                number_processes=number_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-radiation",
   "metadata": {},
   "source": [
    "#### 2.2.2. Wavelet Decomposition<a name=\"waveletDecomp\"></a>\n",
    "\n",
    "Now, we do a wavelet decomposition of the test set events and project them into the feature space obtained with the training set. First, **write** in `saved_test_wavelets_path`  the path to the folder where to save the wavelets. Then, **write** in `path_saved_eigendecomp` the path to the folder where the training set eigendecomposition was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_test_wavelets_path = directories['intermediate_files_directory']\n",
    "path_saved_eigendecomp = directories['analysis_directory']+'/../example_dataset_aug/wavelet_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-debate",
   "metadata": {},
   "source": [
    "Additionally, **write** the path to the folder where to save the reduced wavelets (`path_saved_reduced_wavelets`) and **choose** the number of PCA components to keep (`number_comps`). *Note* that this value *must* be the same as the one used to obtain the training set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_reduced_wavelets = directories['features_directory']\n",
    "number_comps = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-collectible",
   "metadata": {},
   "source": [
    "Perform the wavelet decomposition and project previously calculated wavelet features onto a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = snfeatures.WaveletFeatures(output_root=saved_test_wavelets_path)\n",
    "\n",
    "test_reduced_wavelet_features = wf.compute_reduced_features(\n",
    "    dataset, number_comps=number_comps, \n",
    "    path_saved_eigendecomp = path_saved_eigendecomp,\n",
    "    **{'wavelet_name': 'sym2', 'number_decomp_levels': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-actor",
   "metadata": {},
   "source": [
    "#### 2.2.3. Include Redshift Information<a name=\"addZ\"></a>\n",
    "\n",
    "**Include** any other features that was used to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = test_reduced_wavelet_features.copy()  # only the wavelet features\n",
    "\n",
    "metadata = dataset.metadata\n",
    "features['hostgal_photoz'] = metadata.hostgal_photoz.values.astype(float)\n",
    "features['hostgal_photoz_err'] = metadata.hostgal_photoz_err.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-niagara",
   "metadata": {},
   "source": [
    "#### 2.2.4. Save the Features<a name=\"saveFeatures\"></a>\n",
    "\n",
    "**Write** in `saved_features_path` the path to the folder where to save the final set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_features = directories['features_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_pickle(os.path.join(path_saved_features, 'features.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-secretariat",
   "metadata": {},
   "source": [
    "#### 2.2.5. Load the Features<a name=\"loadFeatures\"></a> <font color=salmon>(Optional)</font>\n",
    "\n",
    "If you jumped to here, **write** in `analysis_name` the name of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'example_test_dataset_gapless50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = get_directories(folder_path, analysis_name) \n",
    "path_saved_features = directories['features_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, analysis_name+'.pckl')\n",
    "dataset = load_dataset(data_path)  # gapless dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle(os.path.join(path_saved_features, 'features.pckl')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-regulation",
   "metadata": {},
   "source": [
    "## 3. Load Classifier<a name=\"loadClassifier\"></a>\n",
    "\n",
    "First, **write** in `path_saved_classifier` the path to the folder that contains the trained classifier instance. The classifier was trained and saved in [6_train_classifier](6_train_classifier.ipynb). Additionally, **write** in `classifier_name` the name under which the classifier was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_classifier = directories['analysis_directory']+'/../example_dataset_aug/classifications'\n",
    "classifier_name = 'our_classifier.pck'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-anaheim",
   "metadata": {},
   "source": [
    "Load classifier instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_saved_classifier, classifier_name), 'rb') as input:\n",
    "    classifier_instance = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-dealing",
   "metadata": {},
   "source": [
    "Obtain the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_instance.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-while",
   "metadata": {},
   "source": [
    "## 4. Classify Test Set<a name=\"classify\"></a>\n",
    "\n",
    "Compute the predicted class (`y_pred`) and the probability of belonging to each different class (`y_probs`). Note that the predicted class is the one whose probability of belong is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(features)\n",
    "y_probs_test = classifier.predict_proba(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-comparative",
   "metadata": {},
   "source": [
    "## 5. Performance<a name=\"performance\"></a> <font color=salmon>(Optional)</font>\n",
    "\n",
    "If we know the true class label of each event we can calculate the performance of the classifier. Otherwise, our predictions are saved in `y_pred_test` and `y_probs_test`.\n",
    "\n",
    "In this example we know the true class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset.metadata.true_target  # class label of each event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-malawi",
   "metadata": {},
   "source": [
    "### 5.1. Metrics<a name=\"metrics\"></a>\n",
    "\n",
    "We start by computing the Area under the ROC Curve (AUC) and the PLAsTiCC logloss. For that, choose which class to consider as *positive* (the other classes will be considered *negative*). Then, **write** in `which_column` the column that corresponds to that class. Note that the class order is accessed through the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier._classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_column = 2  # we are interested in SN Ia vs others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-mobility",
   "metadata": {},
   "source": [
    "Obtain the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.which_column = which_column\n",
    "auc_test = snclassifier.auc_score(classifier=classifier, X_features=features, \n",
    "                                  y_true=data_labels, which_column=which_column)\n",
    "logloss_test = snclassifier.logloss_score(classifier=classifier, X_features=features, \n",
    "                                          y_true=data_labels)\n",
    "print('{:^10} {:^10} {:^10}'.format('', 'AUC', 'Logloss'))\n",
    "print('{:^10} {:^10.3f} {:^10.3f}'.format('test', auc_test, logloss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-allen",
   "metadata": {},
   "source": [
    "Check how many events we correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pred_right = y_pred_test == data_labels\n",
    "np.sum(is_pred_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-insured",
   "metadata": {},
   "source": [
    "### 5.2. Confusion Matrix<a name=\"cm\"></a>\n",
    "\n",
    "Now, plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Confusion matrix'\n",
    "analysis.plot_confusion_matrix(data_labels, y_pred_test, normalise='accuracy', title=title,\n",
    "                               dict_label_to_real=analysis.dict_label_to_real_plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-print",
   "metadata": {},
   "source": [
    "### 5.3. ROC Curves<a name=\"roc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_classifier_roc_curve(data_labels, y_probs_test,\n",
    "                                   dict_label_to_real=analysis.dict_label_to_real_plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-implement",
   "metadata": {},
   "source": [
    "### 5.4. Recall and Precision<a name=\"recallPrecision\"></a>\n",
    "\n",
    "First we create some auxiliary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_true_snii = data_labels == 42\n",
    "is_true_snibc = data_labels == 62\n",
    "is_true_snia = data_labels == 90\n",
    "\n",
    "is_pred_snii = y_pred_test == 42\n",
    "is_pred_snibc = y_pred_test == 62\n",
    "is_pred_snia = y_pred_test == 90\n",
    "\n",
    "# Use the same class order for the two lists below\n",
    "is_true_type_list = [is_true_snia, is_true_snibc, is_true_snii] \n",
    "is_pred_type_list = [is_pred_snia, is_pred_snibc, is_pred_snii]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-empire",
   "metadata": {},
   "source": [
    "As an example, we will plot the recall and precision as a function of the light curve length.\n",
    "\n",
    "We start by calculating the light curve length for all test set events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_length = analysis.compute_lc_length(dataset)\n",
    "quantity = lc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-columbus",
   "metadata": {},
   "source": [
    "**Choose** bins for the plot and whether or not to plot the values in the middle of the bins. Additionally, to consider only a subset of events, **mask** those events in `extra_subset`. If `extra_subset = True`, all the events are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 280, 6) \n",
    "use_mid_bins = True  # plot using the middle of the bins\n",
    "extra_subset = True  # use all events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-battle",
   "metadata": {},
   "source": [
    "**Write** in `sn_order` an ordered list of the names of the classes. This should correspond to the class order used in `is_true_type_list`. Additionally, you can provide the colours with which to plot the classes results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_order = ['SN Ia', 'SN Ibc', 'SN II']\n",
    "diverg_color = sns.color_palette(\"Set2\", 3, desat=1)\n",
    "sn_colors = [diverg_color[2], diverg_color[1], diverg_color[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-sense",
   "metadata": {},
   "source": [
    "#### 5.4.1. Recall plot<a name=\"recall\"></a>\n",
    "\n",
    "Start by calculating the recall and bootstrapped confidence intervals. Then, plot the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_s, boot_recall_ci, number_in_bin_s = analysis.compute_recall_values(\n",
    "    quantity=quantity, bins=bins, is_pred_right=is_pred_right, \n",
    "    use_mid_bins=use_mid_bins, is_true_type_list=is_true_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-milwaukee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_mid_bins:\n",
    "    mid_bins = (bins[:-1]+bins[1:])/2\n",
    "    show_bins = mid_bins\n",
    "else:\n",
    "    show_bins = bins\n",
    "\n",
    "x_label = 'Light curve length (days)'\n",
    "x_min, x_max = -.5, 247\n",
    "\n",
    "analysis.plot_sne_has_something(\n",
    "    something_s=recall_s, boot_has_something_ci=boot_recall_ci,\n",
    "    bins=show_bins, is_true_type_list=is_true_type_list, \n",
    "    sn_order=sn_order, **{'colors': sn_colors})\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel(x_label)\n",
    "plt.xlim(x_min, x_max) \n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(handletextpad=.4, borderaxespad=.3, handlelength=1,\n",
    "           labelspacing=.2, borderpad=.3, columnspacing=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-return",
   "metadata": {},
   "source": [
    "#### 5.4.2. Precision plot<a name=\"precision\"></a>\n",
    "\n",
    "Start by calculating the precision and bootstrapped confidence intervals. Then, plot the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_s, boot_precision_ci, number_in_bin_s = analysis.compute_precision_values(\n",
    "    quantity=quantity, bins=bins, is_pred_right=is_pred_right, \n",
    "    use_mid_bins=use_mid_bins, is_pred_type_list=is_pred_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-piano",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_mid_bins:\n",
    "    mid_bins = (bins[:-1]+bins[1:])/2\n",
    "    show_bins = mid_bins\n",
    "else:\n",
    "    show_bins = bins\n",
    "\n",
    "x_label = 'Light curve length (days)'\n",
    "x_min, x_max = -.5, 247\n",
    "\n",
    "analysis.plot_sne_has_something(\n",
    "    something_s=precision_s, boot_has_something_ci=boot_precision_ci,\n",
    "    bins=show_bins, is_true_type_list=is_true_type_list, \n",
    "    sn_order=sn_order, **{'colors': sn_colors})\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel(x_label)\n",
    "plt.xlim(x_min, x_max) \n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(handletextpad=.4, borderaxespad=.3, handlelength=1,\n",
    "           labelspacing=.2, borderpad=.3, columnspacing=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-macedonia",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "*Previous notebook:* [6_train_classifier](6_train_classifier.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
