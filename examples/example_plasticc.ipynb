{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running the snmachine pipeline on PLAsTiCC simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of the `snmachine` supernova classification package by classifying a subset simulated data from the photometric light curve astronomical time-series classification challenge (PLAsTiCC). \n",
    "\n",
    "See Lochner et al. (2016): http://arxiv.org/abs/1603.00882\n",
    "\n",
    "NOTE: The results here differ from the paper because we're using a much smaller dataset (in the interest of disk space and speed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"snmachine-plasticc.png\" width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the pipeline we use for this work. However, all the steps are modular and can be replaced. It is easy to write your own algorithm into the pipeline and change any of the stpes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snmachine import example_data, sndata, gps, snaugment, snfeatures, snclassifier, analysis, tsne_plot\n",
    "from utils.plasticc_pipeline import create_folder_structure, get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "sns.set(font_scale=1.3, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from snmachine import sndata, snfeatures, snclassifier, tsne_plot, example_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, pywt,subprocess\n",
    "from sklearn.decomposition import PCA\n",
    "from astropy.table import Table,join,vstack\n",
    "from astropy.io import fits\n",
    "import sklearn.metrics \n",
    "import sncosmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout \n",
    "#I use this to supress unnecessary warnings for clarity\n",
    "%load_ext autoreload\n",
    "%autoreload #Use this to reload modules if they are changed on disk while the notebook is running\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up output structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make lots of output files so it makes sense to put them in one place. This is the recommended output file structure.\n",
    "\n",
    "We start by **naming** our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'plasticc'\n",
    "analysis_name = os.path.join(f'output_{dataset_name}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we **write** in `folder_path` the path where to save the folder.\n",
    "\n",
    "Note that `MultiNest` uses a hardcoded character limit for the output file names (we believe it is 100 characters). Thus, avoid making this file path to lengthy if using nested sampling otherwise the multinest output file names will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_structure(folder_path, analysis_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the folder structure. We save:\n",
    "* the extracted features in `directories['features_directory']`\n",
    "* the classification probabilities and ROC curves in `directories['classifications_directory']`\n",
    "* the any intermediate files (such as multinest chains or GP fits) in `directories['intermediate_files_directory']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = get_directories(folder_path, analysis_name) \n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_features = directories['features_directory']\n",
    "path_saved_interm = directories['intermediate_files_directory']\n",
    "path_saved_classifier = directories['classifications_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PLAsTiCC training data (hosted https://doi.org/10.5281/zenodo.2539456). For that, we **write** the path where the data is, and the name of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../snmachine/example_data'\n",
    "\n",
    "data_file_name = 'plasticc_train_lightcurves.csv'\n",
    "metadata_file_name = 'plasticc_train_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes ~2min to run\n",
    "dataset = sndata.PlasticcData(folder=data_path, data_file=data_file_name,\n",
    "                              metadata_file=metadata_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we restrict ourselves to three supernova types: Ia (90), II (42) and Ibc (62)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dataset.metadata\n",
    "is_snia = metadata.target == 90  # SN Ia\n",
    "is_snibc = metadata.target == 62  # SN Ibc\n",
    "is_snii = metadata.target == 42  # SN II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_to_keep = metadata['object_id'][is_snia | is_snibc | is_snii]\n",
    "objs_to_keep = np.array(objs_to_keep).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the dataset to only contain those SNe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update_dataset(objs_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can plot all the data and cycle through it (left and right arrows on your keyboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each light curve is represented in the Dataset object as an astropy table, compatible with `sncosmo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data[dataset.object_names[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the maximum duration of the gap to allowed in the light curves, `max_gap_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gap_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove all the gaps longer than `max_gap_length`, the `remove_gaps` function must be called a few times; it only removes the first gap longer than `max_gap_length`.\n",
    "\n",
    "To introduce uniformity in the dataset, the resulting light curves are translated so their first observation is at time zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_gaps(max_gap_length*2, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length*2, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)\n",
    "dataset.remove_gaps(max_gap_length, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Gaussian Processes\n",
    "\n",
    "Set the path to the folder to save the GP files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose**:\n",
    "- `t_min`: minimim time to evaluate the Gaussian Process Regression at.\n",
    "- `t_max`: maximum time to evaluate the Gaussian Process Regression at.\n",
    "- `gp_dim`: dimension of the Gaussian Process Regression. If  `gp_dim` is 1, the filters are fitted independently. If `gp_dim` is 2, the Matern kernel is used to fit light curves both in time and wavelength.\n",
    "- `number_gp`: number of points to evaluate the Gaussian Process Regression at.\n",
    "- `number_processes`: number of processors to use for parallelisation (**<font color=green>optional</font>**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = 0\n",
    "t_max = 278\n",
    "\n",
    "gp_dim = 2\n",
    "number_gp = 276\n",
    "number_processes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps.compute_gps(dataset, number_gp=number_gp, t_min=t_min, t_max=t_max, \n",
    "                gp_dim=gp_dim, output_root=path_saved_gps, \n",
    "                number_processes=number_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Dataset\n",
    "\n",
    "Our data augmentation requires previously fitted Gaussian Processes. So, if this was not done before, it needs to be done at this stage.\n",
    "\n",
    "### Choose the Events to Augment<a name=\"chooseEvent\"></a>\n",
    "\n",
    "**Write** in `aug_obj_names` a list containing all the events to augment. Here we will try to augment them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_obj_names = dataset.object_names  # try to augment all events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create** a dictionary that associates to each event, the target number of synthetic events to create from it. Note that some augmentations will fail so this is not the final number of events. Additionally, each class has a different creation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "is_to_aug = np.in1d(dataset.object_names, aug_obj_names)\n",
    "\n",
    "# Choose the target number of events in the augmented dataset. \n",
    "# Usually, only half of this number are accepted in the augmented dataset\n",
    "target_number_aug = np.sum(is_to_aug) * 2\n",
    "\n",
    "number_objs_per_label = collections.Counter(dataset.labels[is_to_aug])\n",
    "number_aug_per_label = target_number_aug//len(number_objs_per_label.keys())\n",
    "objs_number_to_aug = {}\n",
    "for label in number_objs_per_label.keys():\n",
    "    is_label = dataset.labels[is_to_aug] == label\n",
    "    aug_is_label_obj_names = aug_obj_names[is_label]\n",
    "    number_aug_per_obj = number_aug_per_label // np.sum(is_label)\n",
    "    number_extra_aug_per_obj = number_aug_per_label % np.sum(is_label)\n",
    "    extra_obj = np.random.choice(aug_is_label_obj_names, size=number_extra_aug_per_obj, \n",
    "                                 replace=False)\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj for obj in aug_is_label_obj_names})\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj+1 for obj in extra_obj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We aim to create up to {sum(objs_number_to_aug.values())} events.')  # confirm how many events to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Photometric Redshift<a name=\"choosePhotoZ\"></a>\n",
    "\n",
    "In order to simulate realistic photometric redshifts for the synthetic events, following [Boone (2019)](https://iopscience.iop.org/article/10.3847/1538-3881/ab5182) we chose a random event from the test set events that had a spectroscopic redshift measurement, and calculated the difference between its spectroscopic and photometric redshifts. We then added this difference to the true redshift of the augmented event to generate a photometric redshift. \n",
    "\n",
    "**Add** such a dataset containing spectroscopic and photometric redshift, and photometric redshift error of events as `z_table`. If none is provided, a similar table is generated from the events in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a small portion of the test data in interest of time\n",
    "test_data_file_name = 'example_test_dataset.pckl'  \n",
    "test_data_path = os.path.join(data_path, test_data_file_name)\n",
    "\n",
    "test_data = load_dataset(test_data_path)\n",
    "test_metadata = test_data.metadata\n",
    "\n",
    "# Discard the events without spectroscopic redshift; \n",
    "# these are encoded with `hostgal_specz` equal to -9\n",
    "z_table = test_metadata[test_metadata.hostgal_specz > -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Augmentation<a name=\"aug\"></a>\n",
    "\n",
    "We also need to choose which survey to emulate in the augmentation. At the moment `snmachine` contains the Wide-Fast-Deep (WFD) and the Deep Drilling Field (DDF) survey of the Rubin Observatory Legacy Survey of Space and Time. Use `snaugment.PlasticcWFDAugment` for the former survey and `snaugment.PlasticcDDFAugment` for the latter.\n",
    "You can also implement your own augmentation using those classes as an example.\n",
    "\n",
    "In addition to the above inputs, we **chose** the random seed (`random_seed`) used to allow reproducible results and the maximum duration of the augmented light curves (`max_duration`).\n",
    "\n",
    "The value of `max_duration` must be higher than the maximum duration of any light curve in `dataset`. If none is provided, `max_duration` is set to the length of the longest event in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 \n",
    "max_duration = 278 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we augmented following the WFD survey. This step in the notebook takes ~25min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = snaugment.PlasticcWFDAugment(dataset=dataset, path_saved_gps=path_saved_gps, \n",
    "                                   objs_number_to_aug=objs_number_to_aug,\n",
    "                                   random_seed=random_seed, max_duration=max_duration, \n",
    "                                   z_table=z_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug.augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance augmented set\n",
    "\n",
    "We now create a balanced augmented training set with the same number of events from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_new_train_data = aug.only_new_dataset\n",
    "only_new_meta = only_new_train_data.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_42 = only_new_meta.target == 42\n",
    "is_62 = only_new_meta.target == 62\n",
    "is_90 = only_new_meta.target == 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_balance = min(np.sum(is_42), np.sum(is_62), np.sum(is_90))\n",
    "size_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "objs_to_keep = []\n",
    "\n",
    "is_sniabcii = [is_42, is_62, is_90]\n",
    "for i, is_sn in enumerate(is_sniabcii):\n",
    "    indexes = np.where(is_sn)[0]\n",
    "    try:\n",
    "        indexes_to_stay = np.random.choice(\n",
    "            indexes, size=size_balance, replace=False)\n",
    "    except ValueError:\n",
    "        print(f'The class {only_new_meta.target[is_sn][0]} only has {len(indexes)} events.')\n",
    "        indexes_to_stay = indexes\n",
    "    objs_to_stay = is_sn[indexes_to_stay].index.to_numpy()\n",
    "    objs_to_keep.append(objs_to_stay)\n",
    "objs_to_keep = np.concatenate(objs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata = only_new_meta.loc[objs_to_keep]\n",
    "only_new_train_data.object_names = list(new_metadata.index)\n",
    "only_new_train_data.update_dataset(list(new_metadata.index))\n",
    "only_new_train_data.update_dataset(list(only_new_train_data.metadata.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of events per SNe class\n",
    "collections.Counter(only_new_train_data.metadata['target'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_dir = folder_path\n",
    "aug_file_name = 'aug_plasticc.pckl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(augmented_data_dir, aug_file_name), 'wb') as f:\n",
    "    pickle.dump(only_new_train_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract useful features from the data. This can often take a long time, depending on the feature extraction method, so it's a good idea to save these to file.\n",
    "\n",
    "We create a new folder to hold the results relative to the augmented training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = aug_file_name[:-5]\n",
    "create_folder_structure(folder_path, analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine these variables because we will not need the older definitions\n",
    "directories = get_directories(folder_path, analysis_name) \n",
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gaussian Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gps.compute_gps(only_new_train_data, number_gp=number_gp, \n",
    "                t_min=t_min, t_max=t_max, \n",
    "                gp_dim=gp_dim, output_root=path_saved_gps, \n",
    "                number_processes=number_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Decomposition<a name=\"waveletDecomp\"></a>\n",
    "\n",
    "Now, we do a wavelet decomposition of the events. **Write** in `path_saved_wavelets` the path to the folder where to save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_wavelets = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [Lochner et al. (2016)](https://iopscience.iop.org/article/10.3847/0067-0049/225/2/31), we then reduced the dimensionality of this wavelet space using Principal Component Analysis (PCA). Therefore, **choose** the number of PCA components to keep (`number_comps`) and **write** the path to the folder where to save the reduced wavelets (`path_saved_reduced_wavelets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_comps = 40\n",
    "path_saved_reduced_wavelets = directories['features_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the wavelet decomposition and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = snfeatures.WaveletFeatures(output_root=path_saved_wavelets)\n",
    "\n",
    "reduced_wavelet_features = wf.compute_reduced_features(\n",
    "    only_new_train_data, number_comps=number_comps, \n",
    "    **{'wavelet_name': 'sym2', 'number_decomp_levels': 2,\n",
    "       'path_save_eigendecomp': path_saved_reduced_wavelets})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.save_reduced_features(reduced_wavelet_features, path_saved_reduced_wavelets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include Redshift Information<a name=\"addZ\"></a>\n",
    "\n",
    "In [paper]() we found that photometric redshift and its uncertainty are crucial for classification. Therefore, in the cell bellow, we include these properties as features. **Modify** it to include other properties as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduced_wavelet_features.copy()  # only the wavelet features\n",
    "\n",
    "metadata = only_new_train_data.metadata\n",
    "features['hostgal_photoz'] = metadata.hostgal_photoz.values.astype(float)\n",
    "features['hostgal_photoz_err'] = metadata.hostgal_photoz_err.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the features, **write** in `saved_features_path` the path to the folder where to save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved_features = directories['features_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_pickle(os.path.join(path_saved_features, 'features.pckl'))\n",
    "\n",
    "data_labels = only_new_train_data.labels.astype(int)  # class label of each event\n",
    "data_labels.to_pickle(os.path.join(path_saved_features, 'data_labels.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make a t-SNE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(2,3))\n",
    "#tsne_plot.plot(features, data_labels) # Not yet implemented for PLAsTiCC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the machine learning algorithm. There's a utility function in the `snclassifier` library to make it easy to run all the algorithms available, including converting features to `numpy` arrays and rescaling them and automatically generating ROC curves and metrics. Hyperparameters are automatically selected using a grid search combined with cross-validation. All functionality can also be individually run from `snclassifier`.\n",
    "\n",
    "Here we chose a LightGBM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_instance = snclassifier.LightGBMClassifier(classifier_name='our_classifier', random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train and use the classifier generated above or optimise it beforehand. In general, it is important to optimise the classifier hyperparameters.\n",
    "\n",
    "In the paper we use the default optimisation but in interest of time, we do here a shorter optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'learning_rate': [.1, .25, .5]}\n",
    "\n",
    "classifier_instance.optimise(features, data_labels, param_grid=param_grid, \n",
    "                             scoring='logloss', \n",
    "                             number_cv_folds=5, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "\n",
    "We use a small set of the test set here. \n",
    "\n",
    "Before classifying the test set events, we need to obtain their features. \n",
    "\n",
    "#### Preprocess light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.remove_gaps(max_gap_length*2, verbose=True)\n",
    "test_data.remove_gaps(max_gap_length*2, verbose=True)\n",
    "test_data.remove_gaps(max_gap_length, verbose=True)\n",
    "test_data.remove_gaps(max_gap_length, verbose=True)\n",
    "test_data.remove_gaps(max_gap_length, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Features\n",
    "\n",
    "Fit Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_analysis_name = test_data_file_name[:-5]\n",
    "create_folder_structure(folder_path, test_analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directories = get_directories(folder_path, test_analysis_name) \n",
    "test_path_saved_gps = test_directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps.compute_gps(test_data, number_gp=number_gp, t_min=t_min, t_max=t_max, \n",
    "                gp_dim=gp_dim, output_root=test_path_saved_gps, \n",
    "                number_processes=number_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do a wavelet decomposition of the test set events and project them into the feature space obtained with the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path_saved_wavelets = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = snfeatures.WaveletFeatures(output_root=test_path_saved_wavelets)\n",
    "\n",
    "test_reduced_wavelet_features = wf.compute_reduced_features(\n",
    "    test_data, number_comps=number_comps, \n",
    "    path_saved_eigendecomp=path_saved_reduced_wavelets,\n",
    "    **{'wavelet_name': 'sym2', 'number_decomp_levels': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include Redshift Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_reduced_wavelet_features.copy()  # only the wavelet features\n",
    "\n",
    "test_metadata = test_data.metadata\n",
    "test_features['hostgal_photoz'] = test_metadata.hostgal_photoz.values.astype(float)\n",
    "test_features['hostgal_photoz_err'] = test_metadata.hostgal_photoz_err.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify Test Set\n",
    "\n",
    "Compute the predicted class (`y_pred`) and the probability of belonging to each different class (`y_probs`). Note that the predicted class is the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_instance.classifier\n",
    "y_pred_test = classifier.predict(test_features)\n",
    "y_probs_test = classifier.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "If we know the true class label of each event we can calculate the performance of the classifier. Otherwise, our predictions are saved in `y_pred_test` and `y_probs_test`.\n",
    "\n",
    "In this example we know the true class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_labels = test_metadata.true_target  # class label of each event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing the Area under the ROC Curve (AUC) and the PLAsTiCC logloss. For that, choose which class to consider as *positive* (the other classes will be considered *negative*). Then, **write** in `which_column` the column that corresponds to that class. Note that the class order is accessed through the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_column = 2  # we are interested in SN Ia vs others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.which_column = which_column\n",
    "auc_test = snclassifier.auc_score(classifier=classifier, X_features=test_features, \n",
    "                                  y_true=test_data_labels, which_column=which_column)\n",
    "logloss_test = snclassifier.logloss_score(classifier=classifier, X_features=test_features, \n",
    "                                          y_true=test_data_labels)\n",
    "print('{:^10} {:^10} {:^10}'.format('', 'AUC', 'Logloss'))\n",
    "print('{:^10} {:^10.3f} {:^10.3f}'.format('test', auc_test, logloss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Confusion matrix'\n",
    "analysis.plot_confusion_matrix(test_data_labels, y_pred_test, normalise='accuracy', title=title,\n",
    "                               dict_label_to_real=analysis.dict_label_to_real_plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_classifier_roc_curve(test_data_labels, y_probs_test,\n",
    "                                   dict_label_to_real=analysis.dict_label_to_real_plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
