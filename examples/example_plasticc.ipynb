{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running the snmachine pipeline on PLAsTiCC simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of the `snmachine` supernova classification package by classifying a subset simulated data from the photometric light-curve astronomical time-series classification challenge (PLAsTiCC). \n",
    "\n",
    "See Lochner et al. (2016) http://arxiv.org/abs/1603.00882 for the original SPCC-challenge test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the how the pipeline works. As the user, you can choose what feature extraction method you want to use. Here we have three (four, technically, since there are two parametric models) but it's straighforward to write a new feature extraction method. Once features have been extracted, they can be run through one of several machine learning algorithms and again, it's easy to write your own algorithm into the pipeline. There's a convenience function in `snclassifier` to run a feature set through multiple algorithms and plot the result. The rest of this notebook goes through applying each of the feature extraction methods to a set of simulations and running all feature sets through different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout \n",
    "#I use this to supress unnecessary warnings for clarity\n",
    "%load_ext autoreload\n",
    "%autoreload #Use this to reload modules if they are changed on disk while the notebook is running\n",
    "from snmachine import sndata, snfeatures, snclassifier, tsne_plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, pywt,subprocess\n",
    "from sklearn.decomposition import PCA\n",
    "from astropy.table import Table,join,vstack\n",
    "from astropy.io import fits\n",
    "import sklearn.metrics \n",
    "import sncosmo\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of processes you want to use throughout the notebook\n",
    "nproc = 4\n",
    "# Also please specify Data root\n",
    "rt=os.path.join('~/software/data/plasticc_all','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up output structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make lots of output files so it makes sense to put them in one place. This is the recommended output file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='plasticc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING...\n",
    "#Multinest uses a hardcoded character limit for the output file names. I believe it's a limit of 100 characters\n",
    "#so avoid making this file path to lengthy if using nested sampling or multinest output file names will be truncated\n",
    "\n",
    "#Change outdir to somewhere on your computer if you like\n",
    "outdir=os.path.join('output_{}_no_z'.format(dataset),'')\n",
    "out_features=os.path.join(outdir,'features') #Where we save the extracted features to\n",
    "out_class=os.path.join(outdir,'classifications') #Where we save the classification probabilities and ROC curves\n",
    "out_int=os.path.join(outdir,'int') #Any intermediate files (such as multinest chains or GP fits)\n",
    "\n",
    "subprocess.call(['mkdir',outdir])\n",
    "subprocess.call(['mkdir',out_features])\n",
    "subprocess.call(['mkdir',out_class])\n",
    "subprocess.call(['mkdir',out_int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a subset of the PLAsTiCC simulated data (https://arxiv.org/abs/1810.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat=sndata.plasticc_data(folder=rt,data_file='training_set.csv',meta_file='training_set_metadata.csv',from_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types, note these are internal snmachine datatypes\n",
    "types=dat.get_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can plot all the data and cycle through it (left and right arrows on your keyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each light curve is represented in the Dataset object as an astropy table, compatible with `sncosmo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.data[dat.object_names[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract useful features from the data. This can often take a long time, depending on the feature extraction method, so it's a good idea to save these to file (`snmachine` by default saves to astropy tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_file=False #We can use this flag to quickly rerun from saved features\n",
    "run_name=os.path.join(out_features,'{}_all'.format(dataset))\n",
    "read_from_pickle=False\n",
    "pickle_location = rt\n",
    "restart_from_GP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wavelet feature extraction process is quite complicated, although it is fairly fast. Remember to save the PCA eigenvalues, vectors and mean for later reconstruction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveFeats=snfeatures.WaveletFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "if read_from_file:\n",
    "    wave_features=Table.read('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    #Crucial for this format of id's\n",
    "    blah=wave_features['Object'].astype(str)\n",
    "    wave_features.replace_column('Object', blah)\n",
    "    PCA_vals=np.loadtxt('%s_wavelets_PCA_vals.dat' %run_name)\n",
    "    PCA_vec=np.loadtxt('%s_wavelets_PCA_vec.dat' %run_name)\n",
    "    PCA_mean=np.loadtxt('%s_wavelets_PCA_mean.dat' %run_name)\n",
    "elif read_from_pickle:\n",
    "    print('THIS IS NOT CURRENTLY IMPLEMENTED')\n",
    "    f = open(rt)\n",
    "    wave_features=Table.read('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    #Crucial for this format of id's\n",
    "    blah=wave_features['Object'].astype(str)\n",
    "    wave_features.replace_column('Object', blah)\n",
    "    PCA_vals=np.loadtxt('%s_wavelets_PCA_vals.dat' %run_name)\n",
    "    PCA_vec=np.loadtxt('%s_wavelets_PCA_vec.dat' %run_name)\n",
    "    PCA_mean=np.loadtxt('%s_wavelets_PCA_mean.dat' %run_name)\n",
    "elif restart_from_GP:\n",
    "    wave_features=waveFeats.extract_features(dat,nprocesses=nproc,output_root=rt,save_output='all',restart='gp')\n",
    "    wave_features.write('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    np.savetxt('%s_wavelets_PCA_vals.dat' %run_name,waveFeats.PCA_eigenvals)\n",
    "    np.savetxt('%s_wavelets_PCA_vec.dat' %run_name,waveFeats.PCA_eigenvectors)\n",
    "    np.savetxt('%s_wavelets_PCA_mean.dat' %run_name,waveFeats.PCA_mean)\n",
    "    \n",
    "    PCA_vals=waveFeats.PCA_eigenvals\n",
    "    PCA_vec=waveFeats.PCA_eigenvectors\n",
    "    PCA_mean=waveFeats.PCA_mean\n",
    "else:\n",
    "    wave_features=waveFeats.extract_features(dat,nprocesses=nproc,output_root=out_int,save_output='all')\n",
    "    wave_features.write('%s_wavelets.dat' %run_name, format='ascii')\n",
    "    np.savetxt('%s_wavelets_PCA_vals.dat' %run_name,waveFeats.PCA_eigenvals)\n",
    "    np.savetxt('%s_wavelets_PCA_vec.dat' %run_name,waveFeats.PCA_eigenvectors)\n",
    "    np.savetxt('%s_wavelets_PCA_mean.dat' %run_name,waveFeats.PCA_mean)\n",
    "    \n",
    "    PCA_vals=waveFeats.PCA_eigenvals\n",
    "    PCA_vec=waveFeats.PCA_eigenvectors\n",
    "    PCA_mean=waveFeats.PCA_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.set_model(waveFeats.fit_sn,wave_features,PCA_vec,PCA_mean,0,dat.get_max_length(),dat.filter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tsne_plot.plot(wave_features,join(wave_features,types)['Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the machine learning algorithm. There's a utility function in the `snclassifier` library to make it easy to run all the algorithms available, including converting features to `numpy` arrays and rescaling them and automatically generating ROC curves and metrics. Hyperparameters are automatically selected using a grid search combined with cross-validation. All functionality can also be individually run from `snclassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers can be run in parallel, change this parameter to the number of processors on your machine (we're only running 4 algorithms so it won't help to set this any higher than 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available classifiers \n",
    "print(snclassifier.choice_of_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "clss=snclassifier.run_pipeline(wave_features,types,output_name=os.path.join(out_class,'wavelets'),\n",
    "                          classifiers=['nb','knn','svm','boost_dt', 'boost_rf'], nprocesses=nproc,\n",
    "                              classifiers_for_cm_plots='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
